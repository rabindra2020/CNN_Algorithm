{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import h5py\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_data = h5py.File('MNISTdata.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.float32(MNIST_data['x_train'][:])\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:,0]))\n",
    "x_test  = np.float32(MNIST_data['x_test'][:])\n",
    "y_test  = np.int32(np.array(MNIST_data['y_test'][:,0]))\n",
    "MNIST_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    kernals = {}\n",
    "    output_layer = {}\n",
    "    hppr = {}\n",
    "\n",
    "    def __init__(self, num_iterations, l_rate, stride, padding, \n",
    "    dim_kernal, num_kernals, dim_inputs, len_outputs, input_chanl, batch_size = 1):\n",
    "        # initialize the model parameters, including the first and second layer \n",
    "        # parameters and biases\n",
    "        self.hppr = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_iterations\": num_iterations,\n",
    "            \"l_rate\": l_rate,\n",
    "            \"stride\": stride,\n",
    "            \"padding\": padding,\n",
    "            \"dim_kernal\": dim_kernal,\n",
    "            \"num_kernals\": num_kernals,\n",
    "            \"dim_inputs\" : dim_inputs,\n",
    "            \"len_outputs\" : len_outputs,\n",
    "            \"input_chanl\" : input_chanl\n",
    "        }\n",
    "        temp_dim = dim_inputs - dim_kernal + 1\n",
    "        self.output_layer = {\n",
    "            'para' : np.random.randn(len_outputs, num_kernals, temp_dim, temp_dim) / np.sqrt(temp_dim**2*num_kernals*len_outputs),\n",
    "            'bias' : np.random.randn(len_outputs,1) / np.sqrt(len_outputs)\n",
    "        }\n",
    "        for i in range(num_kernals):\n",
    "            self.kernals[i] = np.random.randn(input_chanl,dim_kernal,dim_kernal) / np.sqrt(dim_kernal**2)\n",
    "        \n",
    "    def printing(self):\n",
    "        print('Hyperparameters')\n",
    "        for i,j in self.kernals.items():\n",
    "            print(i,':',j.shape)\n",
    "        for i,j in self.output_layer.items():\n",
    "            print(i,':',j.shape)\n",
    "        for i,j in self.hppr.items():\n",
    "            print(i,':',j)\n",
    "        print('->->->->->->->->->->')\n",
    "\n",
    "    def activfunc(self,Z,type = 'ReLU',deri = False):\n",
    "        # implement the activation function\n",
    "        if type == 'ReLU':\n",
    "            if deri == True:\n",
    "                return 1*(Z>0)\n",
    "            else:\n",
    "                return Z*(Z>0)\n",
    "        elif type == 'Sigmoid':\n",
    "            if deri == True:\n",
    "                return 1/(1+np.exp(-Z))*(1-1/(1+np.exp(-Z)))\n",
    "            else:\n",
    "                return 1/(1+np.exp(-Z))\n",
    "        elif type == 'tanh':\n",
    "            if deri == True:\n",
    "                return \n",
    "            else:\n",
    "                return 1-(np.tanh(Z))**2\n",
    "        else:\n",
    "            raise TypeError('Invalid type!')\n",
    "\n",
    "    def Softmax(self,z):\n",
    "        # implement the softmax function\n",
    "        return 1/sum(np.exp(z)) * np.exp(z)\n",
    "\n",
    "    def cross_entropy_error(self,v,y):\n",
    "        # implement the cross entropy error\n",
    "        return -np.log(v[y])\n",
    "\n",
    "    def convolution(self,x,kernals):\n",
    "        ''' input -- x: 3D-array of size (num_channels,dim_inputs,dim_inputs) e.g.(3,28,28);\n",
    "                     kernals: a dictionary of kernals e.g. {0:(3,5,5) ... 4:(3,5,5)}\n",
    "            output-- fm: a 3D-array of feature maps of size \n",
    "                     (num_kernals,dim_inputs-dim_kernal+1,dim_inputs-dim_kernal+1) e.g.(5,26,26)\n",
    "        '''\n",
    "        num_kernals = len(kernals)\n",
    "        x_sp = x.shape\n",
    "        k_sp = kernals[0].shape\n",
    "        t_dim = x_sp[1] - k_sp[1] + 1\n",
    "        result = np.zeros((num_kernals,t_dim,t_dim))\n",
    "        for i in range(num_kernals):\n",
    "            for j in range(t_dim):\n",
    "                for k in range(t_dim):\n",
    "                    result[i,j,k] = np.sum(np.multiply(kernals[i],x[:,j:j+k_sp[1],k:k+k_sp[2]]))\n",
    "        return result\n",
    "\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        ''' input -- x: training data input x, size of (784,)\n",
    "                     y: training data output y, integer\n",
    "            output-- a dictionary of Z, H, U, f_X, error\n",
    "        '''\n",
    "        dim = self.hppr['dim_inputs']\n",
    "        X = x.reshape(self.hppr['input_chanl'],dim,dim)\n",
    "        K = self.kernals\n",
    "\n",
    "        temp_dim = self.hppr['dim_inputs'] - self.hppr['dim_kernal'] + 1\n",
    "        Z = self.convolution(X,K)\n",
    "        H = self.activfunc(Z).reshape((temp_dim**2*self.hppr['num_kernals'],1))\n",
    "        U = np.matmul(self.output_layer['para'].reshape((10,temp_dim**2*self.hppr['num_kernals'])),H) + self.output_layer['bias']\n",
    "        predict_list = np.squeeze(self.Softmax(U))\n",
    "        # error = self.cross_entropy_error(predict_list,y)\n",
    "        \n",
    "        dic = {\n",
    "            'Z':Z,\n",
    "            'H':H,\n",
    "            'U':U,\n",
    "            'f_X':predict_list.reshape((1,self.hppr['len_outputs'])),\n",
    "        #    'error':error\n",
    "        }\n",
    "        return dic\n",
    "\n",
    "    def back_propagation(self,x,y,f_result):\n",
    "        ''' input -- x: training data input x, size of (784,)\n",
    "                     y: training data output y, integer\n",
    "                     f_result: a dictionary of Z, H, U, f_X, error\n",
    "            output--\n",
    "        '''\n",
    "        E = np.array([0]*self.hppr['len_outputs']).reshape((1,self.hppr['len_outputs']))\n",
    "        E[0][y] = 1\n",
    "        dU = (-(E - f_result['f_X'])).reshape((self.hppr['len_outputs'],1))\n",
    "        db = copy.copy(dU)\n",
    "\n",
    "        # tmp_dim = self.hppr['dim_inputs']-self.hppr['dim_kernal']+1\n",
    "        delta = np.zeros((self.hppr['num_kernals'],26,26))\n",
    "        for i in range(10):\n",
    "            delta += self.output_layer['para'][i,:]*np.squeeze(dU)[i]\n",
    "        \n",
    "        dW = np.zeros((10,5,26,26))\n",
    "        for i in range(10):\n",
    "            dW[i]=np.squeeze(dU)[i]*f_result['H'].reshape((5,26,26))\n",
    "\n",
    "        dK = {}\n",
    "        for i in range(5):\n",
    "            tmp_dic = {}\n",
    "            for j in range(1):\n",
    "                tmp_dic[j] = np.multiply(f_result['Z'][j],delta[j]).reshape((1,26,26))\n",
    "            dK[i] = self.convolution(x.reshape((1,28,28)),tmp_dic)\n",
    "\n",
    "        \n",
    "        grad = {\n",
    "            'db':db,\n",
    "            'dW':dW,\n",
    "            'dK':dK\n",
    "        }\n",
    "        return grad\n",
    "\n",
    "    def optimize(self,b_result, learning_rate):\n",
    "        # update the hyperparameters\n",
    "        self.output_layer['para'] -= learning_rate*b_result['dW']\n",
    "        self.output_layer['bias'] -= learning_rate*b_result['db']\n",
    "        for i in range(5):\n",
    "            self.kernals[i] -= learning_rate*b_result['dK'][i]\n",
    "\n",
    "    def loss(self,X_test,Y_test):\n",
    "        # implement the loss function of the training set\n",
    "        loss = 0\n",
    "        for n in range(len(X_test)):\n",
    "            if n % 1000 == 0:\n",
    "                print('computing loss',n)\n",
    "            y = Y_test[n]\n",
    "            x = X_test[n][:]\n",
    "            loss += self.forward(x,y)['error']\n",
    "        return loss\n",
    "\n",
    "    def train(self, X_train, Y_train):\n",
    "        # generate a random list of indices for the training set\n",
    "        learning_rate = self.hppr['l_rate']\n",
    "        num_iterations = self.hppr['num_iterations']\n",
    "        rand_indices = np.random.choice(len(X_train), num_iterations, replace=True)\n",
    "        \n",
    "        def l_rate(base_rate, ite, num_iterations, schedule = False):\n",
    "        # determine whether to use the learning schedule\n",
    "            if schedule == True:\n",
    "                return base_rate * 10 ** (-np.floor(ite/num_iterations*4))\n",
    "            else:\n",
    "                return base_rate\n",
    "\n",
    "        count = 1\n",
    "        loss_dict = {}\n",
    "        test_dict = {}\n",
    "\n",
    "        for i in rand_indices:\n",
    "            f_result = self.forward(X_train[i],Y_train[i])\n",
    "            b_result = self.back_propagation(X_train[i],Y_train[i],f_result)\n",
    "            self.optimize(b_result,l_rate(learning_rate,i,num_iterations,False))\n",
    "            \n",
    "            if count % 100 == 0:\n",
    "                if count % 30000 == 0:\n",
    "                    loss = 'NA' # self.loss(x_test,y_test)\n",
    "                    test = self.testing(x_test,y_test)\n",
    "                    print('Trained for {} times,'.format(count),'loss = {}, test = {}'.format(loss,test))\n",
    "                    # loss_dict[str(count)]=loss\n",
    "                    test_dict[str(count)]=test\n",
    "                else:\n",
    "                    print('Trained for {} times,'.format(count))\n",
    "            count += 1\n",
    "\n",
    "        print('Training finished!')\n",
    "        return loss_dict, test_dict\n",
    "\n",
    "    def testing(self,X_test, Y_test):\n",
    "        # test the model on the training dataset\n",
    "        total_correct = 0\n",
    "        for n in range(len(X_test)):\n",
    "            y = Y_test[n]\n",
    "            x = X_test[n][:]\n",
    "            prediction = np.argmax(self.forward(x,y)['f_X'])\n",
    "            if (prediction == y):\n",
    "                total_correct += 1\n",
    "            if n % 1000 == 0:\n",
    "                print('testing data',n)\n",
    "        print('Accuarcy Test: ',total_correct/len(X_test))\n",
    "        return total_correct/np.float(len(X_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data fitting, training and accuracy evaluation\n",
    "model = CNN(batch_size = 1, num_iterations=12, l_rate=0.01, stride=1, \n",
    "padding=0, dim_kernal=3, num_kernals=5, dim_inputs=28, input_chanl=1, len_outputs=10)\n",
    "model.printing()\n",
    "cost_dict,tests_dict = model.train(x_train,y_train)\n",
    "accu = model.testing(x_test,y_test)\n",
    "model.printing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss function and test accuracy corresponding to the number of iterations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tests_dict.keys(),tests_dict.values())\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Test accuracy with respect to  number of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
